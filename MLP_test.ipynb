{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        self.w = torch.randn(num_inputs, num_outputs, requires_grad=True, dtype=torch.float32)\n",
    "        self.b = torch.randn(num_outputs, 1, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.out = (inputs @ self.w + self.b).relu()\n",
    "        return self.out\n",
    "\n",
    "    def train(self, inputs, outputs, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            loss = ((outputs - self.forward(inputs))**2).flatten().sum()\n",
    "            loss.backward()\n",
    "            print(f\"loss: {loss.data}\")\n",
    "            self.w.data = self.w.data - self.w.grad * learning_rate \n",
    "            self.b.data = self.b.data - self.b.grad * learning_rate \n",
    "            self.w.grad = None\n",
    "            self.b.grad = None\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_neurons, inputs_per_neuron): \n",
    "        self.num_neurons = num_neurons\n",
    "        self.w = torch.randn(num_neurons, inputs_per_neuron, requires_grad=True, dtype=torch.float32)\n",
    "        self.b = torch.randn(num_neurons, 1, requires_grad=True, dtype=torch.float32)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = ((self.w @ inputs) + self.b)\n",
    "        return out \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, num_layers, num_inputs, num_neurons_per_layer, num_final_out):\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = [Layer(num_neurons_per_layer, num_inputs)]\n",
    "\n",
    "        for _ in range(num_layers): \n",
    "            layer = Layer(num_neurons_per_layer, num_neurons_per_layer)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        layer_out = Layer(num_final_out, num_neurons_per_layer)\n",
    "        self.layers.append(layer_out)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        out_prev = input\n",
    "        for i in range(len(self.layers)):\n",
    "            if i == (len(self.layers)-1):\n",
    "                out_prev = self.layers[i].forward(out_prev)\n",
    "            else:\n",
    "                out_prev = self.layers[i].forward(out_prev).tanh()\n",
    "        return out_prev\n",
    "        \n",
    "    def train(self, epochs, learning_rate, x, y):\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        for _ in range(epochs):\n",
    "            # loss = ((y - self.forward(x))**2).flatten().sum()\n",
    "            y_pred = self.forward(x).view(y.shape[0], y.shape[1])\n",
    "            # print(y_pred.shape, y_pred.dtype)\n",
    "            loss = loss_func(y_pred, y)\n",
    "            print(f\"Loss: {loss}\")\n",
    "            loss.backward()\n",
    "            for layer in self.layers:\n",
    "                layer.w.data -= learning_rate * layer.w.grad\n",
    "                layer.b.data -= learning_rate * layer.b.grad\n",
    "                layer.w.grad = None\n",
    "                layer.b.grad = None\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.05\n",
    "num_neurons_per_layer = 16\n",
    "num_layers = 2\n",
    "train_split = 0.90\n",
    "batch_size = int(150*train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(batch_size):\n",
    "    headers=[\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"name\"]\n",
    "    data = pd.read_csv(\"/Users/adityatandon/Documents/VS Code/Learn ML/Data/iris.data\", names=headers).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    dataset_size = len(data) # 150\n",
    "    num_classifications = len(set(data.name)) # 3\n",
    "    num_features = len(data.columns) - 1 # 4\n",
    "\n",
    "    rand_seed_idx = torch.randint((dataset_size - batch_size), (1,)).item() #generates a tensor with a random integer and extracts it using .item()\n",
    "    training_data = data[rand_seed_idx : rand_seed_idx+batch_size].reset_index(drop=True, inplace=False)\n",
    "\n",
    "    diff_flowers = set(data.name)\n",
    "    classification = []\n",
    "    for idx in range(len(training_data['name'])):\n",
    "        if training_data.name[idx] == \"Iris-setosa\":\n",
    "            classification.append([1,0,0])\n",
    "        elif training_data.name[idx] == \"Iris-versicolor\":\n",
    "            classification.append([0,1,0])\n",
    "        elif training_data.name[idx] == \"Iris-virginica\":\n",
    "            classification.append([0,0,1])\n",
    "    \n",
    "    training_data['classification'] = classification\n",
    "    features_list=[]\n",
    "    for i in range(num_features):\n",
    "        features_list.append(torch.tensor(training_data.iloc(axis=1)[i].tolist()).reshape(len(training_data.iloc(axis=1)[0].tolist()), 1)) \n",
    "\n",
    "    train_in = torch.cat(features_list, dim=1)\n",
    "    train_out = torch.tensor(training_data['classification'].tolist(), dtype=torch.float32)\n",
    "    train_in = train_in.reshape(batch_size, num_features, 1)\n",
    "    train_out = train_out.reshape(batch_size, num_classifications)\n",
    "\n",
    "    examples = train_split * dataset_size\n",
    "    num_inputs = num_features\n",
    "    num_outputs = num_classifications\n",
    "\n",
    "    return train_in, train_out, num_features, num_classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in, train_out, num_features, num_classifications = datagen(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MLP(num_layers=3, num_inputs=num_features, num_neurons_per_layer=num_neurons_per_layer, num_final_out=num_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = classifier.forward(train_in).view(train_out.shape[0], train_out.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.8150, -1.9397,  8.4740], grad_fn=<SelectBackward0>),\n",
       " torch.Size([135, 3]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0], train_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.882198333740234\n",
      "Loss: 2.6204450130462646\n",
      "Loss: 4.033931732177734\n",
      "Loss: 1.055057406425476\n",
      "Loss: 0.9541016817092896\n",
      "Loss: 0.8764929175376892\n",
      "Loss: 0.8208673596382141\n",
      "Loss: 0.8105874061584473\n",
      "Loss: 0.6726351976394653\n",
      "Loss: 0.6398022770881653\n",
      "Loss: 0.5880043506622314\n",
      "Loss: 0.5603414177894592\n",
      "Loss: 0.5320409536361694\n",
      "Loss: 0.5161989331245422\n",
      "Loss: 0.5053256750106812\n",
      "Loss: 0.4984767735004425\n",
      "Loss: 0.4944321811199188\n",
      "Loss: 0.49199172854423523\n",
      "Loss: 0.4903290867805481\n",
      "Loss: 0.48905205726623535\n",
      "Loss: 0.48800086975097656\n",
      "Loss: 0.48710253834724426\n",
      "Loss: 0.48631545901298523\n",
      "Loss: 0.48561254143714905\n",
      "Loss: 0.48497501015663147\n",
      "Loss: 0.4843895137310028\n",
      "Loss: 0.48384609818458557\n",
      "Loss: 0.48333752155303955\n",
      "Loss: 0.4828581213951111\n",
      "Loss: 0.48240339756011963\n",
      "Loss: 0.4819701015949249\n",
      "Loss: 0.4815555810928345\n",
      "Loss: 0.4811572730541229\n",
      "Loss: 0.4807736873626709\n",
      "Loss: 0.48040324449539185\n",
      "Loss: 0.4800444543361664\n",
      "Loss: 0.47969648241996765\n",
      "Loss: 0.4793583154678345\n",
      "Loss: 0.47902911901474\n",
      "Loss: 0.4787081182003021\n",
      "Loss: 0.4783947467803955\n",
      "Loss: 0.47808849811553955\n",
      "Loss: 0.4777887463569641\n",
      "Loss: 0.47749510407447815\n",
      "Loss: 0.477207213640213\n",
      "Loss: 0.4769245982170105\n",
      "Loss: 0.47664710879325867\n",
      "Loss: 0.4763742983341217\n",
      "Loss: 0.4761059880256653\n",
      "Loss: 0.4758419990539551\n",
      "Loss: 0.47558197379112244\n",
      "Loss: 0.47532597184181213\n",
      "Loss: 0.4750736355781555\n",
      "Loss: 0.4748249053955078\n",
      "Loss: 0.47457969188690186\n",
      "Loss: 0.4743380546569824\n",
      "Loss: 0.47409960627555847\n",
      "Loss: 0.47386452555656433\n",
      "Loss: 0.4736326336860657\n",
      "Loss: 0.4734039604663849\n",
      "Loss: 0.4731783866882324\n",
      "Loss: 0.47295600175857544\n",
      "Loss: 0.4727366864681244\n",
      "Loss: 0.47252050042152405\n",
      "Loss: 0.47230738401412964\n",
      "Loss: 0.4720973074436188\n",
      "Loss: 0.47189027070999146\n",
      "Loss: 0.4716861844062805\n",
      "Loss: 0.4714852273464203\n",
      "Loss: 0.4712872803211212\n",
      "Loss: 0.47109222412109375\n",
      "Loss: 0.47090014815330505\n",
      "Loss: 0.4707110822200775\n",
      "Loss: 0.4705248177051544\n",
      "Loss: 0.47034168243408203\n",
      "Loss: 0.4701612889766693\n",
      "Loss: 0.4699837267398834\n",
      "Loss: 0.4698088765144348\n",
      "Loss: 0.469637006521225\n",
      "Loss: 0.4694679379463196\n",
      "Loss: 0.46930137276649475\n",
      "Loss: 0.46913763880729675\n",
      "Loss: 0.4689764380455017\n",
      "Loss: 0.4688180088996887\n",
      "Loss: 0.4686620235443115\n",
      "Loss: 0.4685087203979492\n",
      "Loss: 0.46835780143737793\n",
      "Loss: 0.4682094156742096\n",
      "Loss: 0.4680633544921875\n",
      "Loss: 0.4679196774959564\n",
      "Loss: 0.46777835488319397\n",
      "Loss: 0.46763935685157776\n",
      "Loss: 0.46750250458717346\n",
      "Loss: 0.4673679769039154\n",
      "Loss: 0.4672354459762573\n",
      "Loss: 0.4671052098274231\n",
      "Loss: 0.46697697043418884\n",
      "Loss: 0.46685081720352173\n",
      "Loss: 0.4667266011238098\n",
      "Loss: 0.46660441160202026\n",
      "Loss: 0.4664839804172516\n",
      "Loss: 0.4663657248020172\n",
      "Loss: 0.4662490487098694\n",
      "Loss: 0.46613436937332153\n",
      "Loss: 0.4660213589668274\n",
      "Loss: 0.4659099578857422\n",
      "Loss: 0.4658004641532898\n",
      "Loss: 0.4656926095485687\n",
      "Loss: 0.4655863344669342\n",
      "Loss: 0.4654816687107086\n",
      "Loss: 0.4653785526752472\n",
      "Loss: 0.4652770459651947\n",
      "Loss: 0.4651769697666168\n",
      "Loss: 0.46507835388183594\n",
      "Loss: 0.46498119831085205\n",
      "Loss: 0.4648854732513428\n",
      "Loss: 0.46479108929634094\n",
      "Loss: 0.46469810605049133\n",
      "Loss: 0.4646064341068268\n",
      "Loss: 0.4645160734653473\n",
      "Loss: 0.4644269049167633\n",
      "Loss: 0.464339017868042\n",
      "Loss: 0.46425241231918335\n",
      "Loss: 0.4641670286655426\n",
      "Loss: 0.4640827178955078\n",
      "Loss: 0.4639996290206909\n",
      "Loss: 0.46391761302948\n",
      "Loss: 0.4638367295265198\n",
      "Loss: 0.4637569785118103\n",
      "Loss: 0.4636782109737396\n",
      "Loss: 0.4636005163192749\n",
      "Loss: 0.463523805141449\n",
      "Loss: 0.4634481966495514\n",
      "Loss: 0.46337345242500305\n",
      "Loss: 0.4632997512817383\n",
      "Loss: 0.46322697401046753\n",
      "Loss: 0.4631550908088684\n",
      "Loss: 0.4630841612815857\n",
      "Loss: 0.4630140960216522\n",
      "Loss: 0.4629449248313904\n",
      "Loss: 0.46287664771080017\n",
      "Loss: 0.46280914545059204\n",
      "Loss: 0.46274253726005554\n",
      "Loss: 0.46267667412757874\n",
      "Loss: 0.46261170506477356\n",
      "Loss: 0.4625474214553833\n",
      "Loss: 0.4624839127063751\n",
      "Loss: 0.46242114901542664\n",
      "Loss: 0.4623592495918274\n",
      "Loss: 0.46229806542396545\n",
      "Loss: 0.46223750710487366\n",
      "Loss: 0.46217766404151917\n",
      "Loss: 0.46211856603622437\n",
      "Loss: 0.4620600938796997\n",
      "Loss: 0.46200239658355713\n",
      "Loss: 0.46194520592689514\n",
      "Loss: 0.46188876032829285\n",
      "Loss: 0.4618328809738159\n",
      "Loss: 0.4617776870727539\n",
      "Loss: 0.4617230296134949\n",
      "Loss: 0.46166911721229553\n",
      "Loss: 0.46161574125289917\n",
      "Loss: 0.46156296133995056\n",
      "Loss: 0.46151062846183777\n",
      "Loss: 0.4614589810371399\n",
      "Loss: 0.461407870054245\n",
      "Loss: 0.46135738492012024\n",
      "Loss: 0.46130725741386414\n",
      "Loss: 0.4612577557563782\n",
      "Loss: 0.4612088203430176\n",
      "Loss: 0.4611603319644928\n",
      "Loss: 0.4611123204231262\n",
      "Loss: 0.4610649049282074\n",
      "Loss: 0.4610179662704468\n",
      "Loss: 0.4609714448451996\n",
      "Loss: 0.4609254002571106\n",
      "Loss: 0.46087998151779175\n",
      "Loss: 0.4608348309993744\n",
      "Loss: 0.4607901871204376\n",
      "Loss: 0.46074599027633667\n",
      "Loss: 0.46070224046707153\n",
      "Loss: 0.4606590270996094\n",
      "Loss: 0.4606160819530487\n",
      "Loss: 0.46057361364364624\n",
      "Loss: 0.460531622171402\n",
      "Loss: 0.4604899287223816\n",
      "Loss: 0.4604487419128418\n",
      "Loss: 0.46040797233581543\n",
      "Loss: 0.46036747097969055\n",
      "Loss: 0.4603274166584015\n",
      "Loss: 0.4602876901626587\n",
      "Loss: 0.4602484703063965\n",
      "Loss: 0.46020951867103577\n",
      "Loss: 0.4601709246635437\n",
      "Loss: 0.46013274788856506\n",
      "Loss: 0.4600948691368103\n",
      "Loss: 0.46005740761756897\n",
      "Loss: 0.4600202143192291\n",
      "Loss: 0.4599834382534027\n",
      "Loss: 0.459946870803833\n",
      "Loss: 0.4599107503890991\n",
      "Loss: 0.4598749577999115\n",
      "Loss: 0.45983943343162537\n",
      "Loss: 0.4598042368888855\n",
      "Loss: 0.4597693681716919\n",
      "Loss: 0.4597347676753998\n",
      "Loss: 0.4597005248069763\n",
      "Loss: 0.4596666097640991\n",
      "Loss: 0.45963290333747864\n",
      "Loss: 0.4595995545387268\n",
      "Loss: 0.4595664441585541\n",
      "Loss: 0.45953360199928284\n",
      "Loss: 0.45950108766555786\n",
      "Loss: 0.4594688415527344\n",
      "Loss: 0.45943689346313477\n",
      "Loss: 0.4594051241874695\n",
      "Loss: 0.45937374234199524\n",
      "Loss: 0.4593425691127777\n",
      "Loss: 0.4593116343021393\n",
      "Loss: 0.45928096771240234\n",
      "Loss: 0.4592505991458893\n",
      "Loss: 0.45922040939331055\n",
      "Loss: 0.4591905176639557\n",
      "Loss: 0.45916077494621277\n",
      "Loss: 0.4591314196586609\n",
      "Loss: 0.45910218358039856\n",
      "Loss: 0.4590732753276825\n",
      "Loss: 0.45904457569122314\n",
      "Loss: 0.4590160548686981\n",
      "Loss: 0.458987832069397\n",
      "Loss: 0.458959698677063\n",
      "Loss: 0.45893189311027527\n",
      "Loss: 0.4589042663574219\n",
      "Loss: 0.4588768184185028\n",
      "Loss: 0.45884969830513\n",
      "Loss: 0.45882266759872437\n",
      "Loss: 0.45879584550857544\n",
      "Loss: 0.4587692618370056\n",
      "Loss: 0.45874297618865967\n",
      "Loss: 0.4587167799472809\n",
      "Loss: 0.4586907923221588\n",
      "Loss: 0.4586649537086487\n",
      "Loss: 0.45863932371139526\n",
      "Loss: 0.45861393213272095\n",
      "Loss: 0.45858871936798096\n",
      "Loss: 0.4585637152194977\n",
      "Loss: 0.45853883028030396\n",
      "Loss: 0.45851415395736694\n",
      "Loss: 0.45848962664604187\n",
      "Loss: 0.4584653079509735\n",
      "Loss: 0.4584411680698395\n",
      "Loss: 0.458417147397995\n",
      "Loss: 0.4583933651447296\n",
      "Loss: 0.4583697021007538\n",
      "Loss: 0.4583461880683899\n",
      "Loss: 0.4583228826522827\n",
      "Loss: 0.4582996964454651\n",
      "Loss: 0.4582767188549042\n",
      "Loss: 0.45825380086898804\n",
      "Loss: 0.458231121301651\n",
      "Loss: 0.4582086503505707\n",
      "Loss: 0.45818620920181274\n",
      "Loss: 0.45816394686698914\n",
      "Loss: 0.45814183354377747\n",
      "Loss: 0.4581198990345001\n",
      "Loss: 0.4580981433391571\n",
      "Loss: 0.45807650685310364\n",
      "Loss: 0.45805490016937256\n",
      "Loss: 0.4580335021018982\n",
      "Loss: 0.45801228284835815\n",
      "Loss: 0.45799121260643005\n",
      "Loss: 0.45797017216682434\n",
      "Loss: 0.45794928073883057\n",
      "Loss: 0.45792850852012634\n",
      "Loss: 0.4579080045223236\n",
      "Loss: 0.45788753032684326\n",
      "Loss: 0.45786720514297485\n",
      "Loss: 0.4578469395637512\n",
      "Loss: 0.45782679319381714\n",
      "Loss: 0.45780688524246216\n",
      "Loss: 0.45778700709342957\n",
      "Loss: 0.45776721835136414\n",
      "Loss: 0.4577476680278778\n",
      "Loss: 0.4577280879020691\n",
      "Loss: 0.4577087163925171\n",
      "Loss: 0.45768943428993225\n",
      "Loss: 0.4576702415943146\n",
      "Loss: 0.45765119791030884\n",
      "Loss: 0.4576322138309479\n",
      "Loss: 0.45761337876319885\n",
      "Loss: 0.457594633102417\n",
      "Loss: 0.4575760066509247\n",
      "Loss: 0.45755741000175476\n",
      "Loss: 0.45753902196884155\n",
      "Loss: 0.45752066373825073\n",
      "Loss: 0.45750242471694946\n",
      "Loss: 0.45748427510261536\n",
      "Loss: 0.4574662446975708\n",
      "Loss: 0.4574483633041382\n",
      "Loss: 0.4574304223060608\n",
      "Loss: 0.4574127197265625\n",
      "Loss: 0.45739495754241943\n",
      "Loss: 0.45737743377685547\n",
      "Loss: 0.4573599398136139\n",
      "Loss: 0.4573425352573395\n",
      "Loss: 0.45732516050338745\n",
      "Loss: 0.45730796456336975\n",
      "Loss: 0.45729076862335205\n",
      "Loss: 0.4572736918926239\n",
      "Loss: 0.45725664496421814\n",
      "Loss: 0.4572398364543915\n",
      "Loss: 0.45722299814224243\n",
      "Loss: 0.4572063088417053\n",
      "Loss: 0.4571896493434906\n",
      "Loss: 0.45717304944992065\n",
      "Loss: 0.45715653896331787\n",
      "Loss: 0.4571400582790375\n",
      "Loss: 0.457123726606369\n",
      "Loss: 0.45710745453834534\n",
      "Loss: 0.4570912718772888\n",
      "Loss: 0.4570750594139099\n",
      "Loss: 0.45705899596214294\n",
      "Loss: 0.45704296231269836\n",
      "Loss: 0.45702704787254333\n",
      "Loss: 0.4570111334323883\n",
      "Loss: 0.4569953382015228\n",
      "Loss: 0.45697957277297974\n",
      "Loss: 0.4569639563560486\n",
      "Loss: 0.45694828033447266\n",
      "Loss: 0.45693275332450867\n",
      "Loss: 0.45691725611686707\n",
      "Loss: 0.45690178871154785\n",
      "Loss: 0.4568864703178406\n",
      "Loss: 0.4568711519241333\n",
      "Loss: 0.456855833530426\n",
      "Loss: 0.4568406343460083\n",
      "Loss: 0.45682552456855774\n",
      "Loss: 0.4568103551864624\n",
      "Loss: 0.4567953944206238\n",
      "Loss: 0.4567803740501404\n",
      "Loss: 0.45676547288894653\n",
      "Loss: 0.4567505717277527\n",
      "Loss: 0.4567357301712036\n",
      "Loss: 0.45672091841697693\n",
      "Loss: 0.4567062258720398\n",
      "Loss: 0.45669153332710266\n",
      "Loss: 0.4566768705844879\n",
      "Loss: 0.4566623270511627\n",
      "Loss: 0.45664775371551514\n",
      "Loss: 0.45663318037986755\n",
      "Loss: 0.4566187560558319\n",
      "Loss: 0.45660433173179626\n",
      "Loss: 0.4565899968147278\n",
      "Loss: 0.4565756320953369\n",
      "Loss: 0.45656126737594604\n",
      "Loss: 0.45654699206352234\n",
      "Loss: 0.45653271675109863\n",
      "Loss: 0.45651859045028687\n",
      "Loss: 0.4565044343471527\n",
      "Loss: 0.45649024844169617\n",
      "Loss: 0.45647621154785156\n",
      "Loss: 0.4564620852470398\n",
      "Loss: 0.45644813776016235\n",
      "Loss: 0.45643407106399536\n",
      "Loss: 0.45642009377479553\n",
      "Loss: 0.45640620589256287\n",
      "Loss: 0.4563922882080078\n",
      "Loss: 0.45637840032577515\n",
      "Loss: 0.45636454224586487\n",
      "Loss: 0.45635080337524414\n",
      "Loss: 0.45633694529533386\n",
      "Loss: 0.4563230872154236\n",
      "Loss: 0.45630934834480286\n",
      "Loss: 0.45629554986953735\n",
      "Loss: 0.4562818706035614\n",
      "Loss: 0.45626822113990784\n",
      "Loss: 0.4562545120716095\n",
      "Loss: 0.45624083280563354\n",
      "Loss: 0.45622718334198\n",
      "Loss: 0.45621350407600403\n",
      "Loss: 0.4561999440193176\n",
      "Loss: 0.45618629455566406\n",
      "Loss: 0.4561727046966553\n",
      "Loss: 0.4561590850353241\n",
      "Loss: 0.4561454951763153\n",
      "Loss: 0.4561319649219513\n",
      "Loss: 0.4561183452606201\n",
      "Loss: 0.4561048448085785\n",
      "Loss: 0.4560912549495697\n",
      "Loss: 0.4560776948928833\n",
      "Loss: 0.4560641944408417\n",
      "Loss: 0.4560506045818329\n",
      "Loss: 0.4560370147228241\n",
      "Loss: 0.4560234546661377\n",
      "Loss: 0.45600998401641846\n",
      "Loss: 0.4559963345527649\n",
      "Loss: 0.45598283410072327\n",
      "Loss: 0.4559692442417145\n",
      "Loss: 0.4559555649757385\n",
      "Loss: 0.4559420943260193\n",
      "Loss: 0.4559285044670105\n",
      "Loss: 0.45591476559638977\n",
      "Loss: 0.45590120553970337\n",
      "Loss: 0.4558875560760498\n",
      "Loss: 0.45587390661239624\n",
      "Loss: 0.4558602273464203\n",
      "Loss: 0.4558464288711548\n",
      "Loss: 0.45583274960517883\n",
      "Loss: 0.4558190107345581\n",
      "Loss: 0.455805242061615\n",
      "Loss: 0.4557914137840271\n",
      "Loss: 0.4557775855064392\n",
      "Loss: 0.45576366782188416\n",
      "Loss: 0.4557498097419739\n",
      "Loss: 0.45573583245277405\n",
      "Loss: 0.4557218849658966\n",
      "Loss: 0.4557078182697296\n",
      "Loss: 0.455693781375885\n",
      "Loss: 0.45567965507507324\n",
      "Loss: 0.4556654989719391\n",
      "Loss: 0.45565134286880493\n",
      "Loss: 0.45563703775405884\n",
      "Loss: 0.4556228220462799\n",
      "Loss: 0.4556083381175995\n",
      "Loss: 0.45559391379356384\n",
      "Loss: 0.4555794596672058\n",
      "Loss: 0.4555648863315582\n",
      "Loss: 0.45555034279823303\n",
      "Loss: 0.4555355906486511\n",
      "Loss: 0.45552074909210205\n",
      "Loss: 0.45550599694252014\n",
      "Loss: 0.45549100637435913\n",
      "Loss: 0.4554760158061981\n",
      "Loss: 0.45546090602874756\n",
      "Loss: 0.45544567704200745\n",
      "Loss: 0.4554302990436554\n",
      "Loss: 0.45541495084762573\n",
      "Loss: 0.45539939403533936\n",
      "Loss: 0.4553838074207306\n",
      "Loss: 0.4553681015968323\n",
      "Loss: 0.45535215735435486\n",
      "Loss: 0.45533624291419983\n",
      "Loss: 0.4553201496601105\n",
      "Loss: 0.45530393719673157\n",
      "Loss: 0.4552875757217407\n",
      "Loss: 0.45527103543281555\n",
      "Loss: 0.45525428652763367\n",
      "Loss: 0.4552375376224518\n",
      "Loss: 0.4552205502986908\n",
      "Loss: 0.4552033543586731\n",
      "Loss: 0.45518597960472107\n",
      "Loss: 0.45516839623451233\n",
      "Loss: 0.4551507532596588\n",
      "Loss: 0.45513278245925903\n",
      "Loss: 0.45511457324028015\n",
      "Loss: 0.45509621500968933\n",
      "Loss: 0.4550776183605194\n",
      "Loss: 0.4550587832927704\n",
      "Loss: 0.45503976941108704\n",
      "Loss: 0.45502039790153503\n",
      "Loss: 0.4550008177757263\n",
      "Loss: 0.45498090982437134\n",
      "Loss: 0.4549606740474701\n",
      "Loss: 0.45494014024734497\n",
      "Loss: 0.4549194276332855\n",
      "Loss: 0.4548982083797455\n",
      "Loss: 0.45487675070762634\n",
      "Loss: 0.4548547863960266\n",
      "Loss: 0.4548325836658478\n",
      "Loss: 0.45480984449386597\n",
      "Loss: 0.45478683710098267\n",
      "Loss: 0.4547632336616516\n",
      "Loss: 0.4547392725944519\n",
      "Loss: 0.45471474528312683\n",
      "Loss: 0.45468974113464355\n",
      "Loss: 0.4546642005443573\n",
      "Loss: 0.4546380639076233\n",
      "Loss: 0.45461133122444153\n",
      "Loss: 0.4545840322971344\n",
      "Loss: 0.4545559883117676\n",
      "Loss: 0.454527348279953\n",
      "Loss: 0.45449790358543396\n",
      "Loss: 0.45446768403053284\n",
      "Loss: 0.45443665981292725\n",
      "Loss: 0.4544048011302948\n",
      "Loss: 0.45437201857566833\n",
      "Loss: 0.4543382525444031\n",
      "Loss: 0.45430341362953186\n",
      "Loss: 0.454267680644989\n",
      "Loss: 0.4542306363582611\n",
      "Loss: 0.4541923403739929\n",
      "Loss: 0.45415276288986206\n",
      "Loss: 0.45411184430122375\n",
      "Loss: 0.4540693461894989\n",
      "Loss: 0.45402535796165466\n",
      "Loss: 0.4539797604084015\n",
      "Loss: 0.45393213629722595\n",
      "Loss: 0.45388269424438477\n",
      "Loss: 0.45383119583129883\n",
      "Loss: 0.45377737283706665\n",
      "Loss: 0.45372122526168823\n",
      "Loss: 0.4536624252796173\n",
      "Loss: 0.4536008834838867\n",
      "Loss: 0.4535362124443054\n",
      "Loss: 0.4534682333469391\n",
      "Loss: 0.4533967077732086\n",
      "Loss: 0.45332133769989014\n",
      "Loss: 0.4532417058944702\n",
      "Loss: 0.4531574547290802\n",
      "Loss: 0.4530680477619171\n",
      "Loss: 0.45297300815582275\n",
      "Loss: 0.4528719484806061\n",
      "Loss: 0.45276400446891785\n",
      "Loss: 0.4526485204696655\n",
      "Loss: 0.45252472162246704\n",
      "Loss: 0.4523916244506836\n",
      "Loss: 0.45224809646606445\n",
      "Loss: 0.45209282636642456\n",
      "Loss: 0.45192447304725647\n",
      "Loss: 0.451741099357605\n",
      "Loss: 0.45154091715812683\n",
      "Loss: 0.45132142305374146\n",
      "Loss: 0.4510798156261444\n",
      "Loss: 0.45081284642219543\n",
      "Loss: 0.4505164921283722\n",
      "Loss: 0.4501858353614807\n",
      "Loss: 0.4498153328895569\n",
      "Loss: 0.44939783215522766\n",
      "Loss: 0.4489249587059021\n",
      "Loss: 0.4483862817287445\n",
      "Loss: 0.44776928424835205\n",
      "Loss: 0.44705861806869507\n",
      "Loss: 0.4462357461452484\n",
      "Loss: 0.4452788233757019\n",
      "Loss: 0.44416186213493347\n",
      "Loss: 0.4428563117980957\n",
      "Loss: 0.4413314461708069\n",
      "Loss: 0.43955814838409424\n",
      "Loss: 0.4375123977661133\n",
      "Loss: 0.4351794421672821\n",
      "Loss: 0.4325551390647888\n",
      "Loss: 0.4296422302722931\n",
      "Loss: 0.4264439642429352\n",
      "Loss: 0.422958642244339\n",
      "Loss: 0.41918015480041504\n",
      "Loss: 0.4150998890399933\n",
      "Loss: 0.4107080101966858\n",
      "Loss: 0.4059930145740509\n",
      "Loss: 0.4009407162666321\n",
      "Loss: 0.39553800225257874\n",
      "Loss: 0.38997647166252136\n",
      "Loss: 0.3940877914428711\n",
      "Loss: 0.9213734269142151\n",
      "Loss: 4.691349983215332\n",
      "Loss: 4.1553635597229\n",
      "Loss: 3.6318540573120117\n",
      "Loss: 3.1180260181427\n",
      "Loss: 2.4803426265716553\n",
      "Loss: 1.646380066871643\n",
      "Loss: 1.1814874410629272\n",
      "Loss: 1.0698541402816772\n",
      "Loss: 1.052855372428894\n",
      "Loss: 1.0416444540023804\n",
      "Loss: 1.0277029275894165\n",
      "Loss: 1.006377100944519\n",
      "Loss: 0.9717400670051575\n",
      "Loss: 0.9312991499900818\n",
      "Loss: 0.9029216766357422\n",
      "Loss: 0.8844975233078003\n",
      "Loss: 0.8693526387214661\n",
      "Loss: 0.8555771708488464\n",
      "Loss: 0.8426681756973267\n",
      "Loss: 0.8304653167724609\n",
      "Loss: 0.8188479542732239\n",
      "Loss: 0.8077360987663269\n",
      "Loss: 0.7970625758171082\n",
      "Loss: 0.7867767810821533\n",
      "Loss: 0.7768393158912659\n",
      "Loss: 0.7672204971313477\n",
      "Loss: 0.7578974962234497\n",
      "Loss: 0.7488529682159424\n",
      "Loss: 0.7400711178779602\n",
      "Loss: 0.7315372824668884\n",
      "Loss: 0.7232366800308228\n",
      "Loss: 0.7151558995246887\n",
      "Loss: 0.7072842121124268\n",
      "Loss: 0.6996164321899414\n",
      "Loss: 0.6921523213386536\n",
      "Loss: 0.6848973035812378\n",
      "Loss: 0.6778598427772522\n",
      "Loss: 0.671050488948822\n",
      "Loss: 0.66447913646698\n",
      "Loss: 0.658154308795929\n",
      "Loss: 0.6520811319351196\n",
      "Loss: 0.6462619304656982\n",
      "Loss: 0.6406953930854797\n",
      "Loss: 0.6353775858879089\n",
      "Loss: 0.6303024888038635\n",
      "Loss: 0.625461995601654\n",
      "Loss: 0.620847225189209\n",
      "Loss: 0.6164487600326538\n",
      "Loss: 0.6122565865516663\n",
      "Loss: 0.608260989189148\n",
      "Loss: 0.6044518947601318\n",
      "Loss: 0.6008197069168091\n",
      "Loss: 0.5973546504974365\n",
      "Loss: 0.5940477252006531\n",
      "Loss: 0.5908895134925842\n",
      "Loss: 0.5878715515136719\n",
      "Loss: 0.5849854946136475\n",
      "Loss: 0.5822233557701111\n",
      "Loss: 0.5795774459838867\n",
      "Loss: 0.5770406126976013\n",
      "Loss: 0.5746061205863953\n",
      "Loss: 0.572267472743988\n",
      "Loss: 0.5700188279151917\n",
      "Loss: 0.5678544044494629\n",
      "Loss: 0.5657690167427063\n",
      "Loss: 0.5637577176094055\n",
      "Loss: 0.5618160963058472\n",
      "Loss: 0.5599395632743835\n",
      "Loss: 0.5581245422363281\n",
      "Loss: 0.5563670992851257\n",
      "Loss: 0.5546639561653137\n",
      "Loss: 0.5530117750167847\n",
      "Loss: 0.551407516002655\n",
      "Loss: 0.549848735332489\n",
      "Loss: 0.5483324527740479\n",
      "Loss: 0.5468563437461853\n",
      "Loss: 0.5454183220863342\n",
      "Loss: 0.5440160632133484\n",
      "Loss: 0.5426476001739502\n",
      "Loss: 0.5413110852241516\n",
      "Loss: 0.5400046706199646\n",
      "Loss: 0.5387266874313354\n",
      "Loss: 0.5374754667282104\n",
      "Loss: 0.5362496376037598\n",
      "Loss: 0.5350475311279297\n",
      "Loss: 0.5338679552078247\n",
      "Loss: 0.5327093005180359\n",
      "Loss: 0.5315704345703125\n",
      "Loss: 0.5304501056671143\n",
      "Loss: 0.5293468832969666\n",
      "Loss: 0.5282596349716187\n",
      "Loss: 0.5271872878074646\n",
      "Loss: 0.526128351688385\n",
      "Loss: 0.5250819325447083\n",
      "Loss: 0.5240463614463806\n",
      "Loss: 0.5230209827423096\n",
      "Loss: 0.5220043063163757\n",
      "Loss: 0.5209947824478149\n",
      "Loss: 0.5199916362762451\n",
      "Loss: 0.5189931392669678\n",
      "Loss: 0.5179982781410217\n",
      "Loss: 0.5170056223869324\n",
      "Loss: 0.5160136818885803\n",
      "Loss: 0.5150213837623596\n",
      "Loss: 0.5140273571014404\n",
      "Loss: 0.5130305290222168\n",
      "Loss: 0.5120299458503723\n",
      "Loss: 0.5110250115394592\n",
      "Loss: 0.5100156664848328\n",
      "Loss: 0.5090022087097168\n",
      "Loss: 0.5079862475395203\n",
      "Loss: 0.5069699883460999\n",
      "Loss: 0.5059573650360107\n",
      "Loss: 0.5049533843994141\n",
      "Loss: 0.5039647817611694\n",
      "Loss: 0.5029992461204529\n",
      "Loss: 0.5020647644996643\n",
      "Loss: 0.5011688470840454\n",
      "Loss: 0.5003174543380737\n",
      "Loss: 0.49951431155204773\n",
      "Loss: 0.4987596869468689\n",
      "Loss: 0.49805140495300293\n",
      "Loss: 0.49738526344299316\n",
      "Loss: 0.496755987405777\n",
      "Loss: 0.49615779519081116\n",
      "Loss: 0.4955856204032898\n",
      "Loss: 0.4950355291366577\n",
      "Loss: 0.49450400471687317\n",
      "Loss: 0.4939882159233093\n",
      "Loss: 0.49348658323287964\n",
      "Loss: 0.4929974675178528\n",
      "Loss: 0.4925195872783661\n",
      "Loss: 0.4920521378517151\n",
      "Loss: 0.49159443378448486\n",
      "Loss: 0.49114561080932617\n",
      "Loss: 0.49070513248443604\n",
      "Loss: 0.4902727007865906\n",
      "Loss: 0.48984742164611816\n",
      "Loss: 0.4894292652606964\n",
      "Loss: 0.489017516374588\n",
      "Loss: 0.48861202597618103\n",
      "Loss: 0.48821231722831726\n",
      "Loss: 0.48781806230545044\n",
      "Loss: 0.4874289631843567\n",
      "Loss: 0.48704466223716736\n",
      "Loss: 0.4866650104522705\n",
      "Loss: 0.48628953099250793\n",
      "Loss: 0.4859180450439453\n",
      "Loss: 0.48555031418800354\n",
      "Loss: 0.48518604040145874\n",
      "Loss: 0.4848250150680542\n",
      "Loss: 0.4844668209552765\n",
      "Loss: 0.4841115176677704\n",
      "Loss: 0.4837586581707001\n",
      "Loss: 0.48340803384780884\n",
      "Loss: 0.4830593466758728\n",
      "Loss: 0.4827124774456024\n",
      "Loss: 0.4823670983314514\n",
      "Loss: 0.4820230007171631\n",
      "Loss: 0.4816799461841583\n",
      "Loss: 0.48133769631385803\n",
      "Loss: 0.48099592328071594\n",
      "Loss: 0.4806545674800873\n",
      "Loss: 0.48031315207481384\n",
      "Loss: 0.4799714684486389\n",
      "Loss: 0.4796293377876282\n",
      "Loss: 0.4792863726615906\n",
      "Loss: 0.4789421856403351\n",
      "Loss: 0.47859665751457214\n",
      "Loss: 0.4782494008541107\n",
      "Loss: 0.47790011763572693\n",
      "Loss: 0.4775484502315521\n",
      "Loss: 0.4771938621997833\n",
      "Loss: 0.4768361449241638\n",
      "Loss: 0.4764750301837921\n",
      "Loss: 0.4761097729206085\n",
      "Loss: 0.47574010491371155\n",
      "Loss: 0.4753655195236206\n",
      "Loss: 0.47498559951782227\n",
      "Loss: 0.47459977865219116\n",
      "Loss: 0.47420740127563477\n",
      "Loss: 0.4738079011440277\n",
      "Loss: 0.47340065240859985\n",
      "Loss: 0.4729851186275482\n",
      "Loss: 0.47256043553352356\n",
      "Loss: 0.4721257984638214\n",
      "Loss: 0.47168052196502686\n",
      "Loss: 0.4712235629558563\n",
      "Loss: 0.4707542061805725\n",
      "Loss: 0.47027117013931274\n",
      "Loss: 0.469773530960083\n",
      "Loss: 0.46925997734069824\n",
      "Loss: 0.4687294661998749\n",
      "Loss: 0.46818041801452637\n",
      "Loss: 0.46761155128479004\n",
      "Loss: 0.4670211970806122\n",
      "Loss: 0.4664076566696167\n",
      "Loss: 0.46576911211013794\n",
      "Loss: 0.46510371565818787\n",
      "Loss: 0.4644090533256531\n",
      "Loss: 0.463683158159256\n",
      "Loss: 0.46292322874069214\n",
      "Loss: 0.46212688088417053\n",
      "Loss: 0.4612910747528076\n",
      "Loss: 0.4604126513004303\n",
      "Loss: 0.45948827266693115\n",
      "Loss: 0.4585144519805908\n",
      "Loss: 0.45748716592788696\n",
      "Loss: 0.4564021825790405\n",
      "Loss: 0.45525529980659485\n",
      "Loss: 0.45404142141342163\n",
      "Loss: 0.4527556300163269\n",
      "Loss: 0.45139235258102417\n",
      "Loss: 0.4499458968639374\n",
      "Loss: 0.4484102129936218\n",
      "Loss: 0.4467789828777313\n",
      "Loss: 0.4450455904006958\n",
      "Loss: 0.4432034492492676\n",
      "Loss: 0.4412456154823303\n",
      "Loss: 0.43916600942611694\n",
      "Loss: 0.43696045875549316\n",
      "Loss: 0.4346528649330139\n",
      "Loss: 0.4325086772441864\n",
      "Loss: 0.4336107671260834\n",
      "Loss: 0.4684036672115326\n",
      "Loss: 0.5959479808807373\n",
      "Loss: 0.6123100519180298\n",
      "Loss: 0.5112811326980591\n",
      "Loss: 0.48954513669013977\n",
      "Loss: 0.4871479272842407\n",
      "Loss: 0.4852112829685211\n",
      "Loss: 0.48355260491371155\n",
      "Loss: 0.48210111260414124\n",
      "Loss: 0.4808107614517212\n",
      "Loss: 0.4796505272388458\n",
      "Loss: 0.4785984754562378\n",
      "Loss: 0.4776381850242615\n",
      "Loss: 0.47675734758377075\n",
      "Loss: 0.4759463369846344\n",
      "Loss: 0.4751974046230316\n",
      "Loss: 0.47450393438339233\n",
      "Loss: 0.47386014461517334\n",
      "Loss: 0.4732607901096344\n",
      "Loss: 0.47270065546035767\n",
      "Loss: 0.47217485308647156\n",
      "Loss: 0.4716785252094269\n",
      "Loss: 0.4712064862251282\n",
      "Loss: 0.4707539975643158\n",
      "Loss: 0.4703160226345062\n",
      "Loss: 0.469887912273407\n",
      "Loss: 0.46946457028388977\n",
      "Loss: 0.46904098987579346\n",
      "Loss: 0.46861210465431213\n",
      "Loss: 0.4681724011898041\n",
      "Loss: 0.467715859413147\n",
      "Loss: 0.46723583340644836\n",
      "Loss: 0.4667245149612427\n",
      "Loss: 0.466172993183136\n",
      "Loss: 0.46557044982910156\n",
      "Loss: 0.464903324842453\n",
      "Loss: 0.46415528655052185\n",
      "Loss: 0.46330514550209045\n",
      "Loss: 0.46232640743255615\n",
      "Loss: 0.461184561252594\n",
      "Loss: 0.4598354697227478\n",
      "Loss: 0.4582226276397705\n",
      "Loss: 0.4562760293483734\n",
      "Loss: 0.4539130628108978\n",
      "Loss: 0.4510486125946045\n",
      "Loss: 0.4476206600666046\n",
      "Loss: 0.44363918900489807\n",
      "Loss: 0.4392434060573578\n",
      "Loss: 0.43470481038093567\n",
      "Loss: 0.4303112030029297\n",
      "Loss: 0.42620402574539185\n",
      "Loss: 0.42234429717063904\n",
      "Loss: 0.4186142385005951\n",
      "Loss: 0.41491013765335083\n",
      "Loss: 0.4111681878566742\n",
      "Loss: 0.4073575437068939\n",
      "Loss: 0.40357017517089844\n",
      "Loss: 0.40205246210098267\n",
      "Loss: 0.4626868963241577\n",
      "Loss: 0.6137808561325073\n",
      "Loss: 0.5124298930168152\n",
      "Loss: 0.4884876310825348\n",
      "Loss: 0.4852789342403412\n",
      "Loss: 0.48285460472106934\n",
      "Loss: 0.4809035062789917\n",
      "Loss: 0.47927290201187134\n",
      "Loss: 0.4778727889060974\n",
      "Loss: 0.47664666175842285\n",
      "Loss: 0.4755561649799347\n",
      "Loss: 0.4745746850967407\n",
      "Loss: 0.4736824035644531\n",
      "Loss: 0.4728645384311676\n",
      "Loss: 0.4721100330352783\n",
      "Loss: 0.4714096486568451\n",
      "Loss: 0.470756471157074\n",
      "Loss: 0.47014448046684265\n",
      "Loss: 0.4695689380168915\n",
      "Loss: 0.4690258502960205\n",
      "Loss: 0.46851134300231934\n",
      "Loss: 0.46802225708961487\n",
      "Loss: 0.46755537390708923\n",
      "Loss: 0.46710747480392456\n",
      "Loss: 0.466675341129303\n",
      "Loss: 0.46625539660453796\n",
      "Loss: 0.46584391593933105\n",
      "Loss: 0.46543675661087036\n",
      "Loss: 0.4650297462940216\n",
      "Loss: 0.4646178185939789\n",
      "Loss: 0.4641954302787781\n",
      "Loss: 0.4637566804885864\n",
      "Loss: 0.4632944166660309\n",
      "Loss: 0.4628003239631653\n",
      "Loss: 0.46226444840431213\n",
      "Loss: 0.46167463064193726\n",
      "Loss: 0.4610152840614319\n",
      "Loss: 0.46026673913002014\n",
      "Loss: 0.459402859210968\n",
      "Loss: 0.4583888649940491\n",
      "Loss: 0.45717689394950867\n",
      "Loss: 0.4557005763053894\n",
      "Loss: 0.45386525988578796\n",
      "Loss: 0.4515359401702881\n",
      "Loss: 0.44851794838905334\n",
      "Loss: 0.4445396065711975\n",
      "Loss: 0.4392554759979248\n",
      "Loss: 0.432340145111084\n",
      "Loss: 0.42380863428115845\n",
      "Loss: 0.4145081341266632\n",
      "Loss: 0.40592339634895325\n",
      "Loss: 0.39879676699638367\n",
      "Loss: 0.3927000164985657\n",
      "Loss: 0.3873019218444824\n",
      "Loss: 0.38710740208625793\n",
      "Loss: 0.552581787109375\n",
      "Loss: 0.6092079281806946\n",
      "Loss: 0.5014222264289856\n",
      "Loss: 0.4865524172782898\n",
      "Loss: 0.4812188148498535\n",
      "Loss: 0.47812381386756897\n",
      "Loss: 0.4760403037071228\n",
      "Loss: 0.47450339794158936\n",
      "Loss: 0.4733045697212219\n",
      "Loss: 0.4723306894302368\n",
      "Loss: 0.4715143144130707\n",
      "Loss: 0.4708133935928345\n",
      "Loss: 0.4701997637748718\n",
      "Loss: 0.4696542024612427\n",
      "Loss: 0.46916285157203674\n",
      "Loss: 0.46871569752693176\n",
      "Loss: 0.4683050811290741\n",
      "Loss: 0.46792516112327576\n",
      "Loss: 0.46757134795188904\n",
      "Loss: 0.4672400653362274\n",
      "Loss: 0.4669283926486969\n",
      "Loss: 0.4666339159011841\n",
      "Loss: 0.46635469794273376\n",
      "Loss: 0.4660890996456146\n",
      "Loss: 0.4658358097076416\n",
      "Loss: 0.46559351682662964\n",
      "Loss: 0.4653613567352295\n",
      "Loss: 0.46513837575912476\n",
      "Loss: 0.4649238884449005\n",
      "Loss: 0.46471714973449707\n",
      "Loss: 0.46451759338378906\n",
      "Loss: 0.4643246829509735\n",
      "Loss: 0.4641379117965698\n",
      "Loss: 0.46395695209503174\n",
      "Loss: 0.4637812674045563\n",
      "Loss: 0.46361055970191956\n",
      "Loss: 0.46344447135925293\n",
      "Loss: 0.4632827341556549\n",
      "Loss: 0.4631248116493225\n",
      "Loss: 0.4629707336425781\n",
      "Loss: 0.462819904088974\n",
      "Loss: 0.46267226338386536\n",
      "Loss: 0.4625275731086731\n",
      "Loss: 0.4623854160308838\n",
      "Loss: 0.4622456133365631\n",
      "Loss: 0.4621078073978424\n",
      "Loss: 0.4619719684123993\n",
      "Loss: 0.46183738112449646\n",
      "Loss: 0.46170416474342346\n",
      "Loss: 0.46157196164131165\n",
      "Loss: 0.4614402949810028\n",
      "Loss: 0.4613091051578522\n",
      "Loss: 0.461177796125412\n",
      "Loss: 0.4610462486743927\n",
      "Loss: 0.4609139859676361\n",
      "Loss: 0.46078068017959595\n",
      "Loss: 0.4606457054615021\n",
      "Loss: 0.46050894260406494\n",
      "Loss: 0.46036961674690247\n",
      "Loss: 0.46022722125053406\n",
      "Loss: 0.4600813090801239\n",
      "Loss: 0.45993104577064514\n",
      "Loss: 0.45977580547332764\n",
      "Loss: 0.45961451530456543\n",
      "Loss: 0.45944666862487793\n",
      "Loss: 0.45927077531814575\n",
      "Loss: 0.4590858221054077\n",
      "Loss: 0.4588903784751892\n",
      "Loss: 0.4586827754974365\n",
      "Loss: 0.4584614038467407\n",
      "Loss: 0.458223819732666\n",
      "Loss: 0.457967609167099\n",
      "Loss: 0.45768964290618896\n",
      "Loss: 0.45738643407821655\n",
      "Loss: 0.4570537209510803\n",
      "Loss: 0.4566861689090729\n",
      "Loss: 0.4562775790691376\n",
      "Loss: 0.45582008361816406\n",
      "Loss: 0.45530417561531067\n",
      "Loss: 0.4547179341316223\n",
      "Loss: 0.4540458023548126\n",
      "Loss: 0.45326870679855347\n",
      "Loss: 0.45236143469810486\n",
      "Loss: 0.4512907862663269\n",
      "Loss: 0.4500133693218231\n",
      "Loss: 0.4484696686267853\n",
      "Loss: 0.44657808542251587\n",
      "Loss: 0.4442247450351715\n",
      "Loss: 0.4412466287612915\n",
      "Loss: 0.43740689754486084\n",
      "Loss: 0.43235957622528076\n",
      "Loss: 0.42561402916908264\n",
      "Loss: 0.4166763722896576\n",
      "Loss: 0.4070269465446472\n",
      "Loss: 0.40127477049827576\n",
      "Loss: 0.39656147360801697\n",
      "Loss: 0.39146462082862854\n",
      "Loss: 0.3858952522277832\n",
      "Loss: 0.3797934353351593\n",
      "Loss: 0.3731047213077545\n",
      "Loss: 0.3657733201980591\n",
      "Loss: 0.3577721118927002\n",
      "Loss: 0.34916624426841736\n",
      "Loss: 0.3402470350265503\n",
      "Loss: 0.3316827714443207\n",
      "Loss: 0.32444632053375244\n",
      "Loss: 0.32232728600502014\n",
      "Loss: 0.3476695418357849\n",
      "Loss: 0.5144320726394653\n",
      "Loss: 0.8441916704177856\n"
     ]
    }
   ],
   "source": [
    "classifier.train(epochs, learning_rate, train_in, train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
