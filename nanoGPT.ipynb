{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), \"Data/Tiny shakespeare/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(sorted(list(set(text)))) \n",
    "data_size = len(text)\n",
    "# Hyperparameters\n",
    "batch_size = 4 #B\n",
    "block_size = 10 #T\n",
    "emb_size = 32 #C\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.has_mps:\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encodings = {}\n",
    "token_decodings = {}\n",
    "for i, token in enumerate(vocab):\n",
    "    token_encodings[token] = i\n",
    "    token_decodings[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(txt):\n",
    "    enc_char = [token_encodings[char] for char in txt]\n",
    "    return enc_char\n",
    "\n",
    "def decode(enc_tokens):\n",
    "    dec_char = [token_decodings[idx] for idx in enc_tokens]\n",
    "    decoded_str = \"\".join(dec_char)\n",
    "    return decoded_str\n",
    "\n",
    "def generate_batch(batch_size, block_size):\n",
    "    idx = torch.randint(0, data_size - block_size - 1, (batch_size,))\n",
    "    data = torch.tensor(\n",
    "        [encode(text[i : i + block_size]) for i in idx], device=device\n",
    "    ) # B x T \n",
    "    targets = torch.tensor(\n",
    "        [encode(text[i + 1 : i + block_size + 1]) for i in idx], device=device\n",
    "    ) # B x T \n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' caps and ', ' were they', 'his chambe', 'urs, the b']\n"
     ]
    }
   ],
   "source": [
    "data, targets = generate_batch(batch_size, block_size)\n",
    "print([decode(data[i].cpu().numpy()) for i in range(data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "head_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.head_size = head_size\n",
    "        self.q = nn.Linear(emb_size, self.head_size, device=device)\n",
    "        self.k = nn.Linear(emb_size, self.head_size, device=device)\n",
    "        self.v = nn.Linear(emb_size, self.head_size, device=device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.q(x) # B, T, C -> B, T, H\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        B, T, H = q.shape\n",
    "        wei = q @ k.transpose(-1, -2) / np.sqrt(self.head_size) # B, T, H @ B, H, T -> B, T, T\n",
    "        # print(wei.shape)\n",
    "        mask = torch.tril(torch.ones(B, T, T)).to(device)\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
    "        wei = nn.functional.softmax(wei, dim=-1)\n",
    "        out = wei @ v # B, T, H  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.emb_size = emb_size\n",
    "        self.head_size = emb_size // n_heads\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for i in range(self.n_heads):\n",
    "            att_head = SelfAttention(self.head_size)\n",
    "            out.append(att_head(x))\n",
    "        # print(len(out), out[0].shape)\n",
    "        logits = torch.cat(out, dim=-1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, emb_size, device=device)\n",
    "        self.pos_emb_table = nn.Embedding(block_size, emb_size, device=device)\n",
    "        self.ff_net = nn.Linear(emb_size, vocab_size, device=device)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.mha = MultiHeadedAttention(self.num_heads)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        token_emb = self.token_emb_table(x) # B, T, C\n",
    "        # print(x.shape, token_emb.shape)\n",
    "        pos_emb = self.pos_emb_table(torch.arange(x.shape[-1], device=device)) # T, C\n",
    "        x = token_emb + pos_emb # B, T, C\n",
    "        x = self.mha(x) # B, T, C\n",
    "        logits = self.ff_net(x) # B, T, vocab_size\n",
    "        # print(logits.shape)\n",
    "        B, T, C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            # targets = self.token_emb_table(targets)\n",
    "            loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_slice = idx[:, -block_size:]\n",
    "            logits, loss = self.forward(idx_slice)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabs = nn.functional.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probabs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "        return decode(idx[0].tolist())\n",
    "\n",
    "\n",
    "    def train(self, num_steps, batch_size):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=3e-4, betas=(0.9, 0.999))\n",
    "        for step in range(num_steps):\n",
    "            optimizer.zero_grad()\n",
    "            data, targets = generate_batch(batch_size, block_size)\n",
    "            logits, loss = self.forward(data, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step+1) % 10 == 0:\n",
    "                print(f\"Step {step}, loss {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = gpt(data, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2502, device='mps:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHRYhDzg LB ,&s qyN,kb:'T b.tsiWXG3:NlWef:FoohM?heS$CkPHa3e\\nt\""
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_tokens=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9, loss 4.026917934417725\n",
      "Step 19, loss 3.9967408180236816\n",
      "Step 29, loss 4.072494029998779\n",
      "Step 39, loss 4.044800281524658\n",
      "Step 49, loss 3.9608895778656006\n",
      "Step 59, loss 3.947067975997925\n",
      "Step 69, loss 4.034868240356445\n",
      "Step 79, loss 3.88858699798584\n",
      "Step 89, loss 3.866732120513916\n",
      "Step 99, loss 3.9286835193634033\n",
      "Step 109, loss 3.988818407058716\n",
      "Step 119, loss 3.93001127243042\n",
      "Step 129, loss 3.9781854152679443\n",
      "Step 139, loss 3.9018938541412354\n",
      "Step 149, loss 3.974118709564209\n",
      "Step 159, loss 3.902158260345459\n",
      "Step 169, loss 4.003384590148926\n",
      "Step 179, loss 3.9022223949432373\n",
      "Step 189, loss 3.9942989349365234\n",
      "Step 199, loss 3.9868481159210205\n",
      "Step 209, loss 3.8934073448181152\n",
      "Step 219, loss 3.9983043670654297\n",
      "Step 229, loss 3.9016008377075195\n",
      "Step 239, loss 3.959432601928711\n",
      "Step 249, loss 3.903341770172119\n",
      "Step 259, loss 3.885266065597534\n",
      "Step 269, loss 3.8942017555236816\n",
      "Step 279, loss 3.904792070388794\n",
      "Step 289, loss 4.027103900909424\n",
      "Step 299, loss 3.875434637069702\n",
      "Step 309, loss 3.8098554611206055\n",
      "Step 319, loss 3.961247444152832\n",
      "Step 329, loss 3.8874306678771973\n",
      "Step 339, loss 3.9558069705963135\n",
      "Step 349, loss 3.958296537399292\n",
      "Step 359, loss 3.9703149795532227\n",
      "Step 369, loss 3.961998224258423\n",
      "Step 379, loss 3.936386823654175\n",
      "Step 389, loss 3.908282518386841\n",
      "Step 399, loss 3.92619252204895\n",
      "Step 409, loss 3.837179660797119\n",
      "Step 419, loss 3.9932525157928467\n",
      "Step 429, loss 3.8419764041900635\n",
      "Step 439, loss 3.995398759841919\n",
      "Step 449, loss 3.923039197921753\n",
      "Step 459, loss 3.9087600708007812\n",
      "Step 469, loss 3.890061855316162\n",
      "Step 479, loss 3.8310351371765137\n",
      "Step 489, loss 3.8772106170654297\n",
      "Step 499, loss 3.8588714599609375\n",
      "Step 509, loss 3.8529694080352783\n",
      "Step 519, loss 3.8868682384490967\n",
      "Step 529, loss 3.9156582355499268\n",
      "Step 539, loss 3.8877251148223877\n",
      "Step 549, loss 3.956397771835327\n",
      "Step 559, loss 3.837263822555542\n",
      "Step 569, loss 3.8881266117095947\n",
      "Step 579, loss 3.867114782333374\n",
      "Step 589, loss 3.789912223815918\n",
      "Step 599, loss 3.9563324451446533\n",
      "Step 609, loss 3.8674259185791016\n",
      "Step 619, loss 3.801131010055542\n",
      "Step 629, loss 3.763404607772827\n",
      "Step 639, loss 3.902580499649048\n",
      "Step 649, loss 3.9275429248809814\n",
      "Step 659, loss 3.7711915969848633\n",
      "Step 669, loss 3.867750883102417\n",
      "Step 679, loss 3.8597252368927\n",
      "Step 689, loss 3.871013879776001\n",
      "Step 699, loss 3.968785285949707\n",
      "Step 709, loss 3.879817008972168\n",
      "Step 719, loss 3.7858219146728516\n",
      "Step 729, loss 3.799891471862793\n",
      "Step 739, loss 3.8652279376983643\n",
      "Step 749, loss 3.8003859519958496\n",
      "Step 759, loss 3.771230459213257\n",
      "Step 769, loss 3.84171986579895\n",
      "Step 779, loss 3.7793614864349365\n",
      "Step 789, loss 3.8666749000549316\n",
      "Step 799, loss 3.904893159866333\n",
      "Step 809, loss 3.8342339992523193\n",
      "Step 819, loss 3.793804407119751\n",
      "Step 829, loss 3.7822465896606445\n",
      "Step 839, loss 3.758375883102417\n",
      "Step 849, loss 3.847233295440674\n",
      "Step 859, loss 3.789388656616211\n",
      "Step 869, loss 3.847292184829712\n",
      "Step 879, loss 3.80889892578125\n",
      "Step 889, loss 3.7331230640411377\n",
      "Step 899, loss 3.817917585372925\n",
      "Step 909, loss 3.7520880699157715\n",
      "Step 919, loss 3.8779027462005615\n",
      "Step 929, loss 3.7192740440368652\n",
      "Step 939, loss 3.876356840133667\n",
      "Step 949, loss 3.7851357460021973\n",
      "Step 959, loss 3.8150501251220703\n",
      "Step 969, loss 3.772712469100952\n",
      "Step 979, loss 3.743607521057129\n",
      "Step 989, loss 3.7214484214782715\n",
      "Step 999, loss 3.699857234954834\n"
     ]
    }
   ],
   "source": [
    "gpt.train(1000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
