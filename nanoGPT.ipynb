{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  Referenced from: <DAC8FDCB-770B-356E-BA9C-E2F40A2AA20E> /opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <AE6DCE26-A528-35ED-BB3D-88890D27E6B9> /opt/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, DistributedSampler\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), \"Data/Tiny shakespeare/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(sorted(list(set(text)))) \n",
    "data_size = len(text)\n",
    "# Hyperparameters\n",
    "batch_size = 1 #B\n",
    "block_size = 8 #T\n",
    "emb_size = 16 #C\n",
    "num_blocks = 1\n",
    "num_heads = 2\n",
    "head_size = 32\n",
    "dropout = 0.2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.has_mps:\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encodings = {}\n",
    "token_decodings = {}\n",
    "for i, token in enumerate(vocab):\n",
    "    token_encodings[token] = i\n",
    "    token_decodings[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(txt):\n",
    "    enc_char = [token_encodings[char] for char in txt]\n",
    "    return enc_char\n",
    "\n",
    "def decode(enc_tokens):\n",
    "    dec_char = [token_decodings[idx] for idx in enc_tokens]\n",
    "    decoded_str = \"\".join(dec_char)\n",
    "    return decoded_str\n",
    "\n",
    "def generate_batch(batch_size, block_size):\n",
    "    idx = torch.randint(0, data_size - block_size - 1, (batch_size,))\n",
    "    data = torch.tensor(\n",
    "        [encode(text[i : i + block_size]) for i in idx], device=device\n",
    "    ) # B x T \n",
    "    targets = torch.tensor(\n",
    "        [encode(text[i + 1 : i + block_size + 1]) for i in idx], device=device\n",
    "    ) # B x T \n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = generate_batch(batch_size, block_size)\n",
    "# print([decode(data[i].cpu().numpy()) for i in range(data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, data_dir, train=True):\n",
    "        super().__init__()\n",
    "        self.dataset = open(data_dir, 'r').read()\n",
    "        # train_dataset, val_dataset = torch.utils.data.random_split(self.data, [int(len(self.data) * 0.8), len(self.data) - int(len(self.data) * 0.8)])\n",
    "        # if train:\n",
    "        #     self.dataset = train_dataset\n",
    "        # else:\n",
    "        #     self.dataset = val_dataset\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # idx = torch.randint(0, data_size - block_size - 1, (batch_size,))\n",
    "        data = torch.tensor(\n",
    "            [encode(self.dataset[i : i + block_size]) for i in idx], device=device\n",
    "        ) # B x T \n",
    "        targets = torch.tensor(\n",
    "            [encode(self.dataset[i + 1 : i + block_size + 1]) for i in idx], device=device\n",
    "        ) # B x T \n",
    "        return data, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShakespeareDataset(data_dir)\n",
    "sampler = DistributedSampler(dataset, num_replicas=1, rank=0)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.head_size = head_size\n",
    "        self.q = nn.Linear(emb_size, self.head_size, device=device)\n",
    "        self.k = nn.Linear(emb_size, self.head_size, device=device)\n",
    "        self.v = nn.Linear(emb_size, self.head_size, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.q(x) # B, T, C -> B, T, H\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        B, T, H = q.shape\n",
    "        wei = q @ k.transpose(-1, -2) / np.sqrt(self.head_size) # B, T, H @ B, H, T -> B, T, T\n",
    "        # print(wei.shape)\n",
    "        mask = torch.tril(torch.ones(B, T, T)).to(device)\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
    "        wei = nn.functional.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v # B, T, H  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, emb_size):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.emb_size = emb_size\n",
    "        self.head_size = emb_size // n_heads\n",
    "        self.linear = nn.Sequential(nn.Linear(emb_size,4 * emb_size), nn.ReLU(), nn.Linear(4 * emb_size, emb_size), nn.Dropout(0.2),)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for i in range(self.n_heads):\n",
    "            att_head = SelfAttention(self.head_size)\n",
    "            out.append(att_head(x))\n",
    "        # print(len(out), out[0].shape)\n",
    "        logits = torch.cat(out, dim=-1)\n",
    "        logits = self.linear(logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, num_blocks, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.mha = MultiHeadedAttention(num_heads, emb_size)\n",
    "        self.ff_net = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_size * 4, emb_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.layer_norm_1 = nn.LayerNorm(emb_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.layer_norm_1(x)) # B, T, C\n",
    "        x = x + self.ff_net(self.layer_norm_2(x)) # B, T, vocab_size\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, emb_size, device=device)\n",
    "        self.pos_emb_table = nn.Embedding(block_size, emb_size, device=device)\n",
    "        # self.ff_net = nn.Linear(emb_size, emb_size, device=device)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        # self.mha = MultiHeadedAttention(self.num_heads)\n",
    "        # self.layer_norm = nn.LayerNorm(emb_size, device=device, dtype=torch.float32)\n",
    "        self.final_ll = nn.Linear(emb_size, vocab_size, device=device)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[FeedForwardBlock(num_blocks, num_heads) for i in range(num_blocks)]\n",
    "        )\n",
    "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, betas=(0.9, 0.95))\n",
    "\n",
    "        dataset = ShakespeareDataset(data_dir)\n",
    "        sampler = DistributedSampler(dataset, num_replicas=1, rank=0)\n",
    "        self.dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        token_emb = self.token_emb_table(x) # B, T, C\n",
    "        # print(x.shape, token_emb.shape)\n",
    "        pos_emb = self.pos_emb_table(torch.arange(x.shape[-1], device=device)) # T, C\n",
    "        x = token_emb + pos_emb # B, T, C\n",
    "#         for _ in range(num_blocks):\n",
    "#             x_res = x\n",
    "#             x = self.layer_norm(x)\n",
    "#             x = self.mha(x) # B, T, C\n",
    "# #             x = x_res + x\n",
    "# #             x_res = x\n",
    "#             x = x_res + self.ff_net(x) # B, T, vocab_size\n",
    "        x = self.blocks(x)\n",
    "        logits = self.final_ll(x)\n",
    "        # print(logits.shape)\n",
    "        B, T, C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            # targets = self.token_emb_table(targets)\n",
    "            loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_slice = idx[:, -block_size:]\n",
    "            logits, loss = self.forward(idx_slice)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabs = nn.functional.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probabs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "        return decode(idx[0].tolist())\n",
    "\n",
    "\n",
    "    def train(self, num_steps, batch_size):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, betas=(0.9, 0.95))\n",
    "        loss_ar = []\n",
    "        for step in range(num_steps):\n",
    "            optimizer.zero_grad()\n",
    "            data, targets = generate_batch(batch_size, block_size)\n",
    "            logits, loss = self.forward(data, targets)\n",
    "            loss_ar.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step+1) % 10 == 0:\n",
    "                print(f\"Step {step}, loss {loss.item()}\")\n",
    "        return loss_ar\n",
    "    \n",
    "    def train_sd(self, num_steps, batch_size):\n",
    "        loss_ar = []\n",
    "        for epoch in range(num_steps):\n",
    "            for step, (data, targets) in enumerate(self.dataloader):\n",
    "                self.optimizer.zero_grad()\n",
    "                logits, loss = self.forward(data, targets)\n",
    "                loss_ar.append(loss.item())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if (step+1) % 10 == 0:\n",
    "                    print(f\"Step {step}, loss {loss.item()}\")\n",
    "        return loss_ar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6593"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([p.numel() for p in gpt.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = gpt(data, targets)\n",
    "generated = gpt.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_tokens=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRMtFBZ,SEwyz!,Ku?!zIaHMvZXFFoahUZTazJORskOpdBZ!upRsUb$iUlmq\n"
     ]
    }
   ],
   "source": [
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9, loss 4.643728256225586\n"
     ]
    }
   ],
   "source": [
    "loss_ar = gpt.train(10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.colorbar(plt.imshow(logits[0].detach().cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLightning(pl.LightningModule):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_emb_table = nn.Embedding(block_size, emb_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.final_ll = nn.Linear(emb_size, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[FeedForwardBlock(num_blocks, num_heads) for i in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        token_emb = self.token_emb_table(x) # B, T, C\n",
    "        # print(x.shape, token_emb.shape)\n",
    "        pos_emb = self.pos_emb_table(torch.arange(x.shape[-1])) # T, C\n",
    "        x = token_emb + pos_emb # B, T, C\n",
    "        x = self.blocks(x)\n",
    "        logits = self.final_ll(x)\n",
    "        # print(logits.shape)\n",
    "        B, T, C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, max_tokens, idx=0):\n",
    "        # idx = encode(idx)\n",
    "        for _ in range(max_tokens):\n",
    "            idx_slice = idx[:, -block_size:]\n",
    "            logits, loss = self.forward(idx_slice)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabs = nn.functional.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probabs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return decode(idx[0].tolist())\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # loss_ar = []\n",
    "        data, targets = batch\n",
    "        data = encode(data)\n",
    "        targets = encode(targets)\n",
    "        logits, loss = self.forward(data, targets)\n",
    "        # loss_ar.append(loss.item())\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f\"Batch_idx {batch_idx}: loss: {loss.item()}\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # loss_ar = []\n",
    "        data, targets = batch\n",
    "        data = encode(data)\n",
    "        targets = encode(targets)\n",
    "        logits, loss = self.forward(data, targets)\n",
    "        # loss_ar.append(loss.item())\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f\"Batch_idx {batch_idx}: loss: {loss.item()}\")\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, betas=(0.9, 0.95))\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, train=True):\n",
    "        super().__init__()\n",
    "        self.data = open(data_dir, \"r\").read()\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(self.data, [int(len(self.data) * 0.8), len(self.data) - int(len(self.data) * 0.8)])\n",
    "        if train:\n",
    "            self.dataset = train_dataset\n",
    "        else:\n",
    "            self.dataset = val_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx = torch.randint(0, data_size - block_size - 1, (batch_size,))\n",
    "        data = torch.tensor(\n",
    "            [self.dataset[i : i + block_size] for i in idx]\n",
    "        )  # B x T\n",
    "        targets = torch.tensor(\n",
    "            [self.dataset[i + 1 : i + block_size + 1] for i in idx]\n",
    "        )  # B x T\n",
    "        return data, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "class ShakespeareDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size, block_size):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.block_size = block_size\n",
    "        self.dataset = None\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = ShakespeareDataset(self.data_dir, train=True)\n",
    "            self.val_dataset = ShakespeareDataset(self.data_dir, train=False)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        assert self.train_dataset is not None, \"Train Dataset is None\"\n",
    "        train_loader =  DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=1,\n",
    "        )\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        assert self.val_dataset is not None, \"Valid Dataset is None\"\n",
    "        val_loader =  DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "        )\n",
    "        return val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type       | Params\n",
      "-----------------------------------------------\n",
      "0 | token_emb_table | Embedding  | 1.0 K \n",
      "1 | pos_emb_table   | Embedding  | 128   \n",
      "2 | final_ll        | Linear     | 1.1 K \n",
      "3 | blocks          | Sequential | 4.3 K \n",
      "-----------------------------------------------\n",
      "6.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.6 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba68aa21295242658a976f7863ee246a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'ShakespeareDataset' on <module '__main__' (built-in)>\n",
      "/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "gpt_l = GPTLightning(vocab_size)\n",
    "shakespeare_dm = ShakespeareDataModule(data_dir=data_dir, batch_size=batch_size, block_size=block_size)\n",
    "shakespeare_dm.setup(stage=\"fit\")\n",
    "\n",
    "trainer = Trainer(accelerator=\"auto\", max_epochs=1, devices=1, )\n",
    "trainer.fit(gpt_l, train_dataloaders=shakespeare_dm.train_dataloader(), val_dataloaders=shakespeare_dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
