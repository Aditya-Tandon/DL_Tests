{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, DistributedSampler\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), \"Data/Tiny shakespeare/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(sorted(list(set(text)))) \n",
    "data_size = len(text)\n",
    "# Hyperparameters\n",
    "batch_size = 1 #B\n",
    "block_size = 8 #T\n",
    "emb_size = 16 #C\n",
    "num_blocks = 1\n",
    "num_heads = 2\n",
    "head_size = 32\n",
    "dropout = 0.2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.has_mps:\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encodings = {}\n",
    "token_decodings = {}\n",
    "for i, token in enumerate(vocab):\n",
    "    token_encodings[token] = i\n",
    "    token_decodings[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(txt):\n",
    "    enc_char = [token_encodings[char] for char in txt]\n",
    "    return enc_char\n",
    "\n",
    "def decode(enc_tokens):\n",
    "    dec_char = [token_decodings[idx] for idx in enc_tokens]\n",
    "    decoded_str = \"\".join(dec_char)\n",
    "    return decoded_str\n",
    "\n",
    "def generate_batch(batch_size, block_size):\n",
    "    idx = torch.randint(0, data_size - block_size - 1, (batch_size,))\n",
    "    data = torch.tensor(\n",
    "        [encode(text[i : i + block_size]) for i in idx], device=device\n",
    "    ) # B x T \n",
    "    targets = torch.tensor(\n",
    "        [encode(text[i + 1 : i + block_size + 1]) for i in idx], device=device\n",
    "    ) # B x T \n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = generate_batch(batch_size, block_size)\n",
    "# print([decode(data[i].cpu().numpy()) for i in range(data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, data_dir, train=True):\n",
    "        super().__init__()\n",
    "        self.data = open(data_dir, 'r').read()\n",
    "        # train_dataset, val_dataset = torch.utils.data.random_split(self.data, [int(len(self.data) * 0.8), len(self.data) - int(len(self.data) * 0.8)])\n",
    "        # if train:\n",
    "        #     self.dataset = train_dataset\n",
    "        # else:\n",
    "        #     self.dataset = val_dataset\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # idx = torch.randint(0, data_size - block_size - 1, (batch_size,))\n",
    "        data = torch.tensor(\n",
    "            [encode(self.dataset[i : i + block_size]) for i in idx], device=device\n",
    "        ) # B x T \n",
    "        targets = torch.tensor(\n",
    "            [encode(self.dataset[i + 1 : i + block_size + 1]) for i in idx], device=device\n",
    "        ) # B x T \n",
    "        return data, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShakespeareDataset(data_dir)\n",
    "sampler = DistributedSampler(dataset, num_replicas=1, rank=0)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.head_size = head_size\n",
    "        self.q = nn.Linear(emb_size, self.head_size, device=device)\n",
    "        self.k = nn.Linear(emb_size, self.head_size, device=device)\n",
    "        self.v = nn.Linear(emb_size, self.head_size, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.q(x) # B, T, C -> B, T, H\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        B, T, H = q.shape\n",
    "        wei = q @ k.transpose(-1, -2) / np.sqrt(self.head_size) # B, T, H @ B, H, T -> B, T, T\n",
    "        # print(wei.shape)\n",
    "        mask = torch.tril(torch.ones(B, T, T)).to(device)\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
    "        wei = nn.functional.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v # B, T, H  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, emb_size):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.emb_size = emb_size\n",
    "        self.head_size = emb_size // n_heads\n",
    "        self.linear = nn.Sequential(nn.Linear(emb_size,4 * emb_size), nn.ReLU(), nn.Linear(4 * emb_size, emb_size), nn.Dropout(0.2),)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for i in range(self.n_heads):\n",
    "            att_head = SelfAttention(self.head_size)\n",
    "            out.append(att_head(x))\n",
    "        # print(len(out), out[0].shape)\n",
    "        logits = torch.cat(out, dim=-1)\n",
    "        logits = self.linear(logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, num_blocks, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.mha = MultiHeadedAttention(num_heads, emb_size)\n",
    "        self.ff_net = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_size * 4, emb_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.layer_norm_1 = nn.LayerNorm(emb_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.layer_norm_1(x)) # B, T, C\n",
    "        x = x + self.ff_net(self.layer_norm_2(x)) # B, T, vocab_size\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, emb_size, device=device)\n",
    "        self.pos_emb_table = nn.Embedding(block_size, emb_size, device=device)\n",
    "        # self.ff_net = nn.Linear(emb_size, emb_size, device=device)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        # self.mha = MultiHeadedAttention(self.num_heads)\n",
    "        # self.layer_norm = nn.LayerNorm(emb_size, device=device, dtype=torch.float32)\n",
    "        self.final_ll = nn.Linear(emb_size, vocab_size, device=device)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[FeedForwardBlock(num_blocks, num_heads) for i in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        token_emb = self.token_emb_table(x) # B, T, C\n",
    "        # print(x.shape, token_emb.shape)\n",
    "        pos_emb = self.pos_emb_table(torch.arange(x.shape[-1], device=device)) # T, C\n",
    "        x = token_emb + pos_emb # B, T, C\n",
    "#         for _ in range(num_blocks):\n",
    "#             x_res = x\n",
    "#             x = self.layer_norm(x)\n",
    "#             x = self.mha(x) # B, T, C\n",
    "# #             x = x_res + x\n",
    "# #             x_res = x\n",
    "#             x = x_res + self.ff_net(x) # B, T, vocab_size\n",
    "        x = self.blocks(x)\n",
    "        logits = self.final_ll(x)\n",
    "        # print(logits.shape)\n",
    "        B, T, C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            # targets = self.token_emb_table(targets)\n",
    "            loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_slice = idx[:, -block_size:]\n",
    "            logits, loss = self.forward(idx_slice)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabs = nn.functional.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probabs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "        return decode(idx[0].tolist())\n",
    "\n",
    "\n",
    "    def train(self, num_steps, batch_size):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, betas=(0.9, 0.95))\n",
    "        loss_ar = []\n",
    "        for step in range(num_steps):\n",
    "            optimizer.zero_grad()\n",
    "            data, targets = generate_batch(batch_size, block_size)\n",
    "            logits, loss = self.forward(data, targets)\n",
    "            loss_ar.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step+1) % 10 == 0:\n",
    "                print(f\"Step {step}, loss {loss.item()}\")\n",
    "        return loss_ar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([p.numel() for p in gpt.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = gpt(data, targets)\n",
    "generated = gpt.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_tokens=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ar = gpt.train(10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.colorbar(plt.imshow(logits[0].detach().cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLightning(pl.LightningModule):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_emb_table = nn.Embedding(block_size, emb_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.final_ll = nn.Linear(emb_size, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[FeedForwardBlock(num_blocks, num_heads) for i in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        token_emb = self.token_emb_table(x) # B, T, C\n",
    "        # print(x.shape, token_emb.shape)\n",
    "        pos_emb = self.pos_emb_table(torch.arange(x.shape[-1])) # T, C\n",
    "        x = token_emb + pos_emb # B, T, C\n",
    "        x = self.blocks(x)\n",
    "        logits = self.final_ll(x)\n",
    "        # print(logits.shape)\n",
    "        B, T, C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, max_tokens, idx=0):\n",
    "        # idx = encode(idx)\n",
    "        for _ in range(max_tokens):\n",
    "            idx_slice = idx[:, -block_size:]\n",
    "            logits, loss = self.forward(idx_slice)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabs = nn.functional.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probabs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return decode(idx[0].tolist())\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # loss_ar = []\n",
    "        data, targets = batch\n",
    "        data = encode(data)\n",
    "        targets = encode(targets)\n",
    "        logits, loss = self.forward(data, targets)\n",
    "        # loss_ar.append(loss.item())\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f\"Batch_idx {batch_idx}: loss: {loss.item()}\")\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, betas=(0.9, 0.95))\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, data_dir, train=True):\n",
    "        super().__init__()\n",
    "        self.data = open(data_dir, \"r\").read()\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(self.data, [int(len(self.data) * 0.8), len(self.data) - int(len(self.data) * 0.8)])\n",
    "        if train:\n",
    "            self.dataset = train_dataset\n",
    "        else:\n",
    "            self.dataset = val_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx = torch.randint(0, data_size - block_size - 1, (batch_size,))\n",
    "        data = torch.tensor(\n",
    "            [self.dataset[i : i + block_size] for i in idx]\n",
    "        )  # B x T\n",
    "        targets = torch.tensor(\n",
    "            [self.dataset[i + 1 : i + block_size + 1] for i in idx]\n",
    "        )  # B x T\n",
    "        return data, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size, block_size):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.block_size = block_size\n",
    "        self.dataset = None\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = ShakespeareDataset(self.data_dir, train=True)\n",
    "        self.val_dataset = ShakespeareDataset(self.data_dir, train=False)\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.dataset = self.train_dataset\n",
    "        else:\n",
    "            self.dataset = self.val_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_loader =  DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "        )\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_loader =  DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "        )\n",
    "        return val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You selected an invalid accelerator name: `accelerator=GPTLightning(\n  (token_emb_table): Embedding(65, 16)\n  (pos_emb_table): Embedding(8, 16)\n  (final_ll): Linear(in_features=16, out_features=65, bias=True)\n  (blocks): Sequential(\n    (0): FeedForwardBlock(\n      (mha): MultiHeadedAttention(\n        (linear): Sequential(\n          (0): Linear(in_features=16, out_features=64, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=64, out_features=16, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (ff_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=16, bias=True)\n        (3): Dropout(p=0.2, inplace=False)\n      )\n      (layer_norm_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      (layer_norm_2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n)`. Available names are: auto, cpu, cuda, hpu, ipu, mps, tpu.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/adityatandon/Documents/VS Code/Deep_Learning/Tests/nanoGPT.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Deep_Learning/Tests/nanoGPT.ipynb#Y150sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m gpt_l \u001b[39m=\u001b[39m GPTLightning(vocab_size)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Deep_Learning/Tests/nanoGPT.ipynb#Y150sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m shakespeare_dm \u001b[39m=\u001b[39m ShakespeareDataModule(data_dir\u001b[39m=\u001b[39mdata_dir, batch_size\u001b[39m=\u001b[39mbatch_size, block_size\u001b[39m=\u001b[39mblock_size)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adityatandon/Documents/VS%20Code/Deep_Learning/Tests/nanoGPT.ipynb#Y150sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(gpt_l, accelerator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, devices\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/utilities/argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m     69\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:399\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39m# init connectors\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector \u001b[39m=\u001b[39m _DataConnector(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 399\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_connector \u001b[39m=\u001b[39m _AcceleratorConnector(\n\u001b[1;32m    400\u001b[0m     devices\u001b[39m=\u001b[39;49mdevices,\n\u001b[1;32m    401\u001b[0m     accelerator\u001b[39m=\u001b[39;49maccelerator,\n\u001b[1;32m    402\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m    403\u001b[0m     num_nodes\u001b[39m=\u001b[39;49mnum_nodes,\n\u001b[1;32m    404\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39;49msync_batchnorm,\n\u001b[1;32m    405\u001b[0m     benchmark\u001b[39m=\u001b[39;49mbenchmark,\n\u001b[1;32m    406\u001b[0m     use_distributed_sampler\u001b[39m=\u001b[39;49muse_distributed_sampler,\n\u001b[1;32m    407\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m    408\u001b[0m     precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[1;32m    409\u001b[0m     plugins\u001b[39m=\u001b[39;49mplugins,\n\u001b[1;32m    410\u001b[0m )\n\u001b[1;32m    411\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger_connector \u001b[39m=\u001b[39m _LoggerConnector(\u001b[39mself\u001b[39m)\n\u001b[1;32m    412\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector \u001b[39m=\u001b[39m _CallbackConnector(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:140\u001b[0m, in \u001b[0;36m_AcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layer_sync: Optional[LayerSync] \u001b[39m=\u001b[39m TorchSyncBatchNorm() \u001b[39mif\u001b[39;00m sync_batchnorm \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_io: Optional[CheckpointIO] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_config_and_set_final_flags(\n\u001b[1;32m    141\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m    142\u001b[0m     accelerator\u001b[39m=\u001b[39;49maccelerator,\n\u001b[1;32m    143\u001b[0m     precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[1;32m    144\u001b[0m     plugins\u001b[39m=\u001b[39;49mplugins,\n\u001b[1;32m    145\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39;49msync_batchnorm,\n\u001b[1;32m    146\u001b[0m )\n\u001b[1;32m    147\u001b[0m \u001b[39m# 2. Instantiate Accelerator\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_accelerator_if_ipu_strategy_is_passed()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:222\u001b[0m, in \u001b[0;36m_AcceleratorConnector._check_config_and_set_final_flags\u001b[0;34m(self, strategy, accelerator, precision, plugins, sync_batchnorm)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    211\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou selected an invalid strategy name: `strategy=\u001b[39m\u001b[39m{\u001b[39;00mstrategy\u001b[39m!r}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m It must be either a string or an instance of `lightning.pytorch.strategies.Strategy`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Example choices: auto, ddp, ddp_spawn, deepspeed, ...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Find a complete list of options in our documentation at https://lightning.ai\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m     )\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    218\u001b[0m     accelerator \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_types\n\u001b[1;32m    219\u001b[0m     \u001b[39mand\u001b[39;00m accelerator \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(accelerator, Accelerator)\n\u001b[1;32m    221\u001b[0m ):\n\u001b[0;32m--> 222\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    223\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou selected an invalid accelerator name: `accelerator=\u001b[39m\u001b[39m{\u001b[39;00maccelerator\u001b[39m!r}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Available names are: auto, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_types)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[39m# MPS accelerator is incompatible with DDP family of strategies. It supports single-device operation only.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m is_ddp_str \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(strategy, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mddp\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m strategy\n",
      "\u001b[0;31mValueError\u001b[0m: You selected an invalid accelerator name: `accelerator=GPTLightning(\n  (token_emb_table): Embedding(65, 16)\n  (pos_emb_table): Embedding(8, 16)\n  (final_ll): Linear(in_features=16, out_features=65, bias=True)\n  (blocks): Sequential(\n    (0): FeedForwardBlock(\n      (mha): MultiHeadedAttention(\n        (linear): Sequential(\n          (0): Linear(in_features=16, out_features=64, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=64, out_features=16, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (ff_net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=16, bias=True)\n        (3): Dropout(p=0.2, inplace=False)\n      )\n      (layer_norm_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      (layer_norm_2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n)`. Available names are: auto, cpu, cuda, hpu, ipu, mps, tpu."
     ]
    }
   ],
   "source": [
    "gpt_l = GPTLightning(vocab_size)\n",
    "shakespeare_dm = ShakespeareDataModule(data_dir=data_dir, batch_size=batch_size, block_size=block_size)\n",
    "trainer = Trainer(gpt_l, accelerator=\"cpu\", max_epochs=1, devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
