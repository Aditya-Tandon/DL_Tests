{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, DistributedSampler\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), \"Data/Tiny shakespeare/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(sorted(list(set(text)))) \n",
    "data_size = len(text)\n",
    "# Hyperparameters\n",
    "batch_size = 100 #B\n",
    "block_size = 200 #T\n",
    "emb_size = 1024 #C\n",
    "num_blocks = 4\n",
    "num_heads = 32\n",
    "head_size = 1024\n",
    "dropout = 0.1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.has_mps:\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encodings = {}\n",
    "token_decodings = {}\n",
    "for i, token in enumerate(vocab):\n",
    "    token_encodings[token] = i\n",
    "    token_decodings[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(txt):\n",
    "    enc_char = [token_encodings[char] for char in txt]\n",
    "    return enc_char\n",
    "\n",
    "def decode(enc_tokens):\n",
    "    dec_char = [token_decodings[idx] for idx in enc_tokens]\n",
    "    decoded_str = \"\".join(dec_char)\n",
    "    return decoded_str\n",
    "\n",
    "def generate_batch(batch_size, block_size):\n",
    "    idx = torch.randint(0, data_size - block_size - 1, (batch_size,))\n",
    "    data = torch.tensor(\n",
    "        [encode(text[i : i + block_size]) for i in idx], device=device\n",
    "    ) # B x T \n",
    "    targets = torch.tensor(\n",
    "        [encode(text[i + 1 : i + block_size + 1]) for i in idx], device=device\n",
    "    ) # B x T \n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = generate_batch(batch_size, block_size)\n",
    "# print([decode(data[i].cpu().numpy()) for i in range(data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, data_dir, train=True):\n",
    "        super().__init__()\n",
    "        self.dataset = open(data_dir, 'r').read()\n",
    "        # train_dataset, val_dataset = torch.utils.data.random_split(self.data, [int(len(self.data) * 0.8), len(self.data) - int(len(self.data) * 0.8)])\n",
    "        # if train:\n",
    "        #     self.dataset = train_dataset\n",
    "        # else:\n",
    "        #     self.dataset = val_dataset\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # idx = torch.randint(0, data_size - block_size - 1, (batch_size,))\n",
    "        data = torch.tensor(\n",
    "            encode(self.dataset[idx : idx + block_size]), device=device\n",
    "        ) # B x T \n",
    "        targets = torch.tensor(\n",
    "            encode(self.dataset[idx + 1 : idx + block_size + 1]), device=device\n",
    "        ) # B x T \n",
    "        return data, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShakespeareDataset(data_dir)\n",
    "sampler = DistributedSampler(dataset, num_replicas=1, rank=0)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.head_size = head_size\n",
    "        self.q = nn.Linear(emb_size, self.head_size, device=device)\n",
    "        self.k = nn.Linear(emb_size, self.head_size, device=device)\n",
    "        self.v = nn.Linear(emb_size, self.head_size, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         print(x.device)\n",
    "        q = self.q(x) # B, T, C -> B, T, H\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        B, T, H = q.shape\n",
    "        wei = q @ k.transpose(-1, -2) / np.sqrt(self.head_size) # B, T, H @ B, H, T -> B, T, T\n",
    "        # print(wei.shape)\n",
    "        mask = torch.tril(torch.ones(B, T, T)).to(x.device)\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
    "        wei = nn.functional.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v # B, T, H  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, emb_size):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.emb_size = emb_size\n",
    "        self.head_size = emb_size // n_heads\n",
    "        self.linear = nn.Sequential(nn.Linear(emb_size,4 * emb_size), nn.ReLU(), nn.Linear(4 * emb_size, emb_size), nn.Dropout(0.2),)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for i in range(self.n_heads):\n",
    "            att_head = SelfAttention(self.head_size)\n",
    "            out.append(att_head(x))\n",
    "        # print(len(out), out[0].shape)\n",
    "        logits = torch.cat(out, dim=-1)\n",
    "        logits = self.linear(logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, num_blocks, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.mha = MultiHeadedAttention(num_heads, emb_size)\n",
    "        self.ff_net = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size * 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(emb_size * 4, emb_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.layer_norm_1 = nn.LayerNorm(emb_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.layer_norm_1(x)) # B, T, C\n",
    "        x = x + self.ff_net(self.layer_norm_2(x)) # B, T, vocab_size\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, emb_size, device=device)\n",
    "        self.pos_emb_table = nn.Embedding(block_size, emb_size, device=device)\n",
    "        # self.ff_net = nn.Linear(emb_size, emb_size, device=device)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        # self.mha = MultiHeadedAttention(self.num_heads)\n",
    "        # self.layer_norm = nn.LayerNorm(emb_size, device=device, dtype=torch.float32)\n",
    "        self.final_ll = nn.Linear(emb_size, vocab_size, device=device)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[FeedForwardBlock(num_blocks, num_heads) for i in range(num_blocks)]\n",
    "        )\n",
    "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, betas=(0.9, 0.95))\n",
    "\n",
    "        dataset = ShakespeareDataset(data_dir)\n",
    "        sampler = DistributedSampler(dataset, num_replicas=1, rank=0, shuffle=True)\n",
    "        self.dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        token_emb = self.token_emb_table(x) # B, T, C\n",
    "        # print(x.shape, token_emb.shape)\n",
    "        pos_emb = self.pos_emb_table(torch.arange(x.shape[-1], device=device)) # T, C\n",
    "        x = token_emb + pos_emb # B, T, C\n",
    "#         for _ in range(num_blocks):\n",
    "#             x_res = x\n",
    "#             x = self.layer_norm(x)\n",
    "#             x = self.mha(x) # B, T, C\n",
    "# #             x = x_res + x\n",
    "# #             x_res = x\n",
    "#             x = x_res + self.ff_net(x) # B, T, vocab_size\n",
    "        x = self.blocks(x)\n",
    "        logits = self.final_ll(x)\n",
    "        # print(logits.shape)\n",
    "        B, T, C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            # targets = self.token_emb_table(targets)\n",
    "            loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_slice = idx[:, -block_size:]\n",
    "            logits, loss = self.forward(idx_slice)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabs = nn.functional.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probabs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "        return decode(idx[0].tolist())\n",
    "\n",
    "\n",
    "    def train(self, num_steps, batch_size):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, betas=(0.9, 0.95))\n",
    "        loss_ar = []\n",
    "        for step in range(num_steps):\n",
    "            optimizer.zero_grad()\n",
    "            data, targets = generate_batch(batch_size, block_size)\n",
    "            logits, loss = self.forward(data, targets)\n",
    "            loss_ar.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step+1) % 10 == 0:\n",
    "                print(f\"Step {step}, loss {loss.item()}\")\n",
    "        return loss_ar\n",
    "    \n",
    "    def train_sd(self, num_epochs, batch_size):\n",
    "        loss_ar = []\n",
    "        for epoch in range(num_epochs):\n",
    "            for step, (data, targets) in enumerate(iter(self.dataloader)):\n",
    "                self.optimizer.zero_grad()\n",
    "                logits, loss = self.forward(data, targets)\n",
    "                loss_ar.append(loss.item())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if (step+1) % 10 == 0:\n",
    "                    print(f\"Step {step}, loss {loss.item()}\")\n",
    "        return loss_ar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([p.numel() for p in gpt.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = gpt(data, targets)\n",
    "generated = gpt.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_tokens=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ar = gpt.train_sd(10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.colorbar(plt.imshow(logits[0].softmax(-1).detach().cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset_lightning(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, train=True):\n",
    "        super().__init__()\n",
    "        total_data = open(data_dir, \"r\").read()\n",
    "        train_data = total_data[:int(0.8*len(total_data))]\n",
    "        val_data = total_data[int(0.8*len(total_data)):]\n",
    "        if train:\n",
    "            self.data = train_data\n",
    "        else:\n",
    "            self.data = val_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if (idx + block_size) >= len(self.data):\n",
    "            idx = torch.randint(0, len(self.data) - block_size - 1, (1,))\n",
    "            \n",
    "        data = torch.tensor(\n",
    "            self.encode(self.data[idx : idx + block_size])\n",
    "        )  # T\n",
    "        targets = torch.tensor(\n",
    "            self.encode(self.data[idx + 1 : idx + block_size + 1])\n",
    "        )  # T\n",
    "#         return {\"data\":data, \"targets\":targets, \"idx\":idx}\n",
    "        return data, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def encode(self, txt):\n",
    "        enc_char = [token_encodings[char] for char in txt]\n",
    "        return enc_char\n",
    "\n",
    "    def decode(self, enc_tokens):\n",
    "        dec_char = [token_decodings[idx] for idx in enc_tokens]\n",
    "        decoded_str = \"\".join(dec_char)\n",
    "        return decoded_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataLoader(DataLoader):\n",
    "    \n",
    "    def __init__(self, dataset, batch_size, shuffle, num_workers, batch_sampler=None, **kwargs):\n",
    "        \n",
    "        if batch_sampler is None:\n",
    "            super().__init__(\n",
    "                dataset=dataset,\n",
    "                batch_size=batch_size, \n",
    "                shuffle=shuffle,\n",
    "                num_workers=num_workers,\n",
    "                collate_fn=self.collate_fn,\n",
    "            )\n",
    "        else:\n",
    "            super().__init__(\n",
    "                dataset=dataset,\n",
    "                batch_size=batch_size,\n",
    "                batch_sampler=batch_sampler,\n",
    "                collate_fn=self.collate_fn,\n",
    "            )\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        # batch --> tuple --> B x T x 2\n",
    "        data = torch.stack([batch[i][0] for i in range(len(batch))])\n",
    "        targets = torch.stack([batch[i][1] for i in range(len(batch))])\n",
    "        return data, targets\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, data_dir, batch_size, block_size):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.block_size = block_size\n",
    "        print(\"initialised\")\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            dataset = ShakespeareDataset_lightning(self.data_dir)\n",
    "            self.train_dataset = ShakespeareDataset_lightning(self.data_dir, train=True)\n",
    "            self.val_dataset = ShakespeareDataset_lightning(self.data_dir, train=False)\n",
    "            print(\"Train and Valid Dataset Loaded\")\n",
    "#         self.train_sampler = RandomSampler(self.train_dataset)\n",
    "#         self.val_sampler = RandomSampler(self.val_dataset)\n",
    "        self.train_sampler = None\n",
    "        self.val_sampler = None\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        assert self.train_dataset is not None, \"Train Dataset is None\"\n",
    "        \n",
    "        data_loader =  ShakespeareDataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=1,\n",
    "            batch_sampler=self.train_sampler,\n",
    "        )\n",
    "        return data_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        assert self.val_dataset is not None, \"Valid Dataset is None\"\n",
    "        \n",
    "        data_loader =  ShakespeareDataLoader(\n",
    "            dataset=self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            batch_sampler=self.val_sampler,\n",
    "        )\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLightning(pl.LightningModule):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, emb_size).to(\"cuda\")\n",
    "        self.pos_emb_table = nn.Embedding(block_size, emb_size).to(\"cuda\")\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.final_ll = nn.Linear(emb_size, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[FeedForwardBlock(num_blocks, num_heads) for i in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x: B, T\n",
    "        token_emb = self.token_emb_table(x) # B, T, C\n",
    "        # print(x.shape, token_emb.shape)\n",
    "        B, T = x.shape\n",
    "        positions = torch.arange(T, device=x.device)\n",
    "#         print(\"x_device: \", x.device)\n",
    "        pos_emb = self.pos_emb_table(positions) # T, C\n",
    "        x = token_emb + pos_emb # B, T, C\n",
    "        x = self.blocks(x)\n",
    "        logits = self.final_ll(x)\n",
    "        # print(logits.shape)\n",
    "        B, T, C = logits.shape\n",
    "        if targets is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits.view(B*T, C), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, targets = batch\n",
    "#         data = encode(data)\n",
    "#         targets = encode(targets)\n",
    "        logits, loss = self.forward(data, targets)\n",
    "        print(f\"Train Batch_idx {batch_idx}; loss: {loss.item()}\")\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         # loss_ar = []\n",
    "#         data, targets = batch\n",
    "# #         data = encode(data)\n",
    "# #         targets = encode(targets)\n",
    "#         logits, loss = self.forward(data, targets)\n",
    "#         # loss_ar.append(loss.item())\n",
    "#         print(f\"Val Batch_idx {batch_idx}; loss: {loss.item()}\")\n",
    "#         return loss\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_slice = idx[:, -block_size:]\n",
    "            logits, loss = self.forward(idx_slice)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabs = nn.functional.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probabs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return decode(idx[0].tolist())\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=5e-5, betas=(0.9, 0.999))\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory():\n",
    "    # Get memory usage of the tensor\n",
    "    curr_memory_used = torch.cuda.memory_allocated(\"cuda\") / (1024 * 1024)  # in MB\n",
    "    max_memory_used = torch.cuda.max_memory_allocated(\"cuda\") / (1024 * 1024)  # in MB\n",
    "    return round(curr_memory_used, 2), round(max_memory_used, 2)\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_l = GPTLightning(vocab_size)\n",
    "shakespeare_dm = ShakespeareDataModule(data_dir=data_dir, \n",
    "                                       batch_size=batch_size, \n",
    "                                       block_size=block_size\n",
    "                                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(accelerator=\"gpu\", \n",
    "                  max_epochs=2, \n",
    "                  val_check_interval=0.01,\n",
    "                  overfit_batches=0,\n",
    "                  limit_val_batches=10,\n",
    "                  num_sanity_val_steps=0,\n",
    "                  devices=1,\n",
    "#                   strategy=\"deepspeed_stage_3\",\n",
    "#                   precision=16,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(gpt_l, datamodule=shakespeare_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt_l.generate(idx=torch.zeros((1,1), device=gpt_l.device, dtype=torch.long) ,max_tokens=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tl = shakespeare_dm.train_dataloader()\n",
    "    batch = next(iter(tl))\n",
    "    batch.to(\"cuda\")\n",
    "    logits, loss = gpt_l.training_step(batch)\n",
    "\n",
    "    plt.colorbar(plt.imshow(logits[0].softmax(dim=-1).detach().cpu().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:esmfold]",
   "language": "python",
   "name": "conda-env-esmfold-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
